# GDSC 2주차

## 1. in-place 연산

이번 과제에서는 in-place 연산에 대하여 알아보도록 하겠습니다. 사실 in-place 연산은 이름 그대로 바꿔치기, 치환 연산이라고 할 수 있습니다. 즉, 어느 연산자가 있을 때 그에 대한 연산을 다른 용어, 문구로 치환하여 사용하는 것입니다.

이러한 치환 연산 방식은 코드를 구현할 때 편의를 추구할 수 있게 해줍니다. 예를 들어 보도록 하겠습니다. 우리가 아래와 같은 연산을 진행한다고 할 때, 

~~~python

~~~



## 2. Array - Tensor 간 변환 방법

Pytorch에서 numpy array를 Tensor 자료형으로 바꾸기 위해서는 `torch.Tensor()`와 `torch.from_numpy()`를 활용할 수 있습니다.

먼저, `torch.Tensor()` 를 [pytorch 공식 문서](https://pytorch.org/docs/stable/tensors.html)에서 확인해본다면 말그대로 Tensor를 만들어내는 구문이기 때문에 나와 있는 예시에는 array가 직접 들어가 있는 경우를 많이 확인할 수 있습니다.

그리고 아래와 같이 생성되는 Tensor의 데이터 타입마다 어떠한 구문을 써야하는지 나타나 있습니다. 아래의 이미지는 그 중 일부를 발췌한 것입니다.

<div align = "center"> <img src = "https://drive.google.com/uc?id=14E34d4n2UhkkVNlCL0EanexuC8gS_hyL" /> </div>



또한, `torch.from_numpy()` 를 [pytorch 공식 문서]("https://pytorch.org/docs/stable/generated/torch.from_numpy.html")에서 확인해본다면 `Creates a Tensor from a numpy.ndarray` 라고 설명되어 있는 것을 확인할 수 있습니다. 말 그대로 numpy로 만들어진 array를 tensor로 만들어주는 역할을 하게 됩니다.

<div align = "center"> <img src="https://drive.google.com/uc?id=1DGSJhwGKGAkVUm478YmIRP25nhycTbHj" /> </div>

다만, 주의해야할 점은 만들어진 tensor는 기존의 ndarray와 동일한 메모리를 공유한다는 점입니다. 그렇기에 tensor에 대한 수정 사항이 ndarray에 반영되고, 그 반대의 경우 또한 적용된다는 점을 고려해야 합니다. 

또한, 그로 인해 만들어진 tensor의 크기는 그 이후 조정할 수 없습니다. 이와 관련된 공식 문서의 설명은 아래와 같습니다. 

<div align = "center"> <img src="https://drive.google.com/uc?id=1VfaE2osqXmezYhioaXN2kLW8VzBCjk4u" /> </div>

그렇다면 정말 작동하는지 확인해봅시다.

먼저, `torch.Tensor()` 함수를 활용하여 numpy array를 Tensor로 바꾸어봅시다.

```python
import numpy as np
import torch

arr = np.array([1,2,3,4,5])
print(type(arr))

arr_tensor = torch.Tensor(arr)
print(type(arr_tensor))
```

아래와 같이 해당하는 type이 변경되는 것을 쉽게 확인할 수 있습니다.

<div align=center> <img src="https://drive.google.com/uc?id=1Wk4zEELWdLsoaOuf-ruHm1Q-V7-I0HEo" /> </div>



다음으로는 `torch.from_numpy()` 함수를 활용하여 numpy array를 Tensor로 바꾸어봅시다.

```python
import numpy as np
import torch

arr2 = np.array([2,3,4])
print(type(arr2))

arr_tensor2 = torch.from_numpy(arr2)
print(type(arr_tensor2))
```

아래와 같이 해당하는 type이 변환되는 것을 확인할 수 있습니다.

<div align=center><img src="https://drive.google.com/uc?id=1Wk4zEELWdLsoaOuf-ruHm1Q-V7-I0HEo" /></div>



이와 반대로 Pytorch에서 tensor에서 numpy로 변경하기 위해서는 `torch.Tensor.numpy()` 구문을 활용할 수 있습니다. 이를 [pytorch 공식문서](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)에서 확인해보면 아래와 같이 간결하게 역할을 확인할 수 있습니다. 

앞선 `torch.from_numpy()` 와 마찬가지로 `torch.Tensor.numpy()` 또한 각 tensor와 ndarray는 같은 메모리를 공유하게 됩니다. 

<div align = "center"> <img src="https://drive.google.com/uc?id=1OGc3E3BBM3DjMMeanVGkkiEGV0v0BgKB" /> </div>

이제는 `numpy()` 를 활용하여 변환된 Tensor를 numpy array로 바꾸어봅시다.

```python
import numpy as np
import torch

arr_numpy = arr_tensor.numpy()
print(type(arr_numpy))
```

아래와 같이 `numpy.ndarray` 타입으로 변환된 것을 확인할 수 있습니다.

<div align=center><img src="https://drive.google.com/uc?id=1p-YqWDZnsikTgAA67YuSpayk6Ko2Qtvo" /></div>



## 3. Tensor Type 정리



## 4. detach(), clone()

결론부터 말하자면, ` detach()`와 `clone()`은 기존에 가지고 있던 Tensor를 복사하는 방법이라고 할 수 있습니다.  각각을 요약하자면 아래과 같이 확인할 수 있습니다.

- `detach()` : 기존 Tensor에서 gradient 전파가 안되는 Tensor 생성
- `clone()` : 기존 Tensor와 내용을 복사한 Tensor 생성



먼저, `detach()`에 대해서 pytorch 공식 문서를 살펴보도록 하겠습니다. 

<div align=center><img src="https://drive.google.com/uc?id=1EjsoONcDpXBIISWx3D6ZINmjn3DGkmSC"/></div>

pytorch 공식 문서에서는 `Returns a new Tensor, detached from the current graph. The result will never require gradient.` 라고 설명하고 있습니다.

이는 graph에서 분리한 새로운 tensor를 반환한다는 말인데, 이는 Pytorch의 특성과 연관되어 있습니다. **Pytorch는 tensor에서 이루어진 모든 연산을 추적해서 기록해놓는다는 특성 (graph)** 이 있습니다. 이러한 연산 기록으로부터 분리한 tensor를 반환한다는 의미입니다. 

<div align=center><img src="https://drive.google.com/uc?id=1lALPTEjoatObEopLAODQ3V3T_6qW1Pfu"/></div>

물론, 위 공식 문서 notice에서처럼 

- 기존 tensor에서 gradient 전파가 안되는 tensor를 생성하게 됩니다.
- 또한, 기존 tensor에서의 storage를 공유하기 때문에 detach로 생성한 Tensor가 변경되면 원본 Tensor가 똑같이 변합니다.

사실, 이러한 방법을 사용하는 이유는 연산기록을 저장한다는 특성에서 찾아볼 수 있습니다. 예를 하나 들어보겠습니다. 



이와 동일한 역할을 하는 구문으로는 `with torch.no_grad() ` 가 있습니다. 다만, 위 방법은 



그 다음으로는 `clone()`에 대하여 pytorch 공식 문서를 살펴보도록 하겠습니다.

<div align=center><img src="https://drive.google.com/uc?id=1-jHeiEUdp-iz6Eob6nFh8eelvIwp8yr0"/></div>



pytorch 공식 문서에서는 `Returns a copy of input`이라는 표현을 사용하는데, 말 그대로 `input tensor`에 대한 복사본을 반환하는 역할을 가진다고 볼 수 있습니다. 

그렇다면, 아까 살펴보았던 `torch.detach()`와 다른 점은 없는지 궁금하실 것입니다.

아래의 공식 문서에서의 notice에서 보듯이 gradient을 받는다는 것을 알 수 있습니다. 

<div align=center><img src="https://drive.google.com/uc?id=1NsMgIJ_DDXEWnotl4uaMVN9N6KC5RLGM"/></div>

위와 같이 parameter도 설정할 수 있는데, 



이렇게 되면 사실 



## 5. contiguous()

