# GDSC 4주차 과제

## 5번. 다른 Normalization이 있을까?

<br/>

이번에 알아볼 내용은 Normalization에 대한 내용입니다. Normalization은 머신러닝과 딥러닝에서 조금은 다른 의미를 가지고 수행되어집니다.

머신러닝에서는 데이터의 scale로 인한 모델의 영향을 줄이기 위하여 사용되며, 딥러닝에서는 gradient vanish 등 학습과정에서 Parameter로 인한 부작용을 제거하여 학습의 정확도를 높이기 위한 목적으로 사용되게 됩니다.

 Normalization의 종류는 매우 많지만, 그 중에서 딥러닝에서 사용되는, Batch Normalization을 제외한 몇 가지만 살펴보고 합니다. 

<br/>

### 1.  Weight Normalization

`Weight Normalization` 은 앞서 커리큘럼에서 살펴보았던 `batch Normalization` 과 달리 layer에서의 Weight (가중치)를 정규화하는 방식을 가지고 있습니다.

<br/>

<div align = "center"> <img src= "https://drive.google.com/uc?id=154cwuOrLOZnNJvh78B7b6olJvi4qJE_T" width="300"> </div>

<br/>

위 사진은 [Weight Normalization: A Simple Reparameterization
to Accelerate Training of Deep Neural Networks](https://arxiv.org/pdf/1602.07868.pdf)라는 논문에서 `Weight Normalization`에 대한 내용을 수식으로 알려주는 사진입니다. 위와 같이 layer의 가중치 w를 변경하게 됩니다. 이 때, `Batch Normalization`에서 입력값을 표준편차로 나누어주는 행동과 비슷한 효과를 얻게 됩니다. 해당 논문에서 `Weight Normalization`은 입력을 표준 편차로 나누거나 scale 재조정 작업을 따로 하지 않는 `mean-only batch normalization`과 같이 사용하여 효과를 확인하는 것을 제안하고 있습니다. 아래는 그에 대한 검증 내용입니다.

<br/>

<div align = "center"> <img src = "https://drive.google.com/uc?id=1881jLKHi3PsEGWQLdSPmuRZ5L8b4cZNW" width="400"> </div>

<br/>


### 2. Layer Normalization
Layer Normalization은 feature 차원에서 진행하는 정규화를 말합니다. 이해를 돕기 위해 아래의 그림을 참고하여 봅시다. 아래의 그림에서는 `Batch Normalization` 과 `Layer Normalization` 의 차이를 보여주고 있습니다. 
<br/>
이 때, `Batch Normalization`은 batch 전체에 대하여 계산이 이루어지고 이는 각 batch에서 동일하게 계산됩니다. 반면, Layer Normalization의 경우 각 feature에 대하여 따로 계산이 이루어지며, 각 feature에 독립적으로 계산합니다. 
<br/>
그렇기에 `Layer Normalization`은 batch 사이즈에 상관이 없으며 특히 RNN에서 효과가 커지는 것을 확인할 수 있습니다.

<div align = "center"> <img src = "https://drive.google.com/uc?id=1KXgV7CtpMd7ovgC1Yzgt5aQryf_IcpFn" width="400"> </div>

<br/>

### 3. Instance Normalization

`Instance Normalization`은 기존의 `Layer Normalization`과 유사하지만 이에 더하여 평균과 표준 편차를 추가적으로 각 example의 각 채널마다 정규화를 시켜주는 방식입니다. 이미지 데이터를 예로 들자면 아래의 사진에서 H, W에만 연산을 진행한다고 볼 수 있습니다. 앞선 예시에서 알 수 있듯이 이는 이미지 데이터에 대하여 국한된 정규화이며, RNN에서는 사용할 수 없어 특정적인 Normalization이라고 할 수 있습니다.

<br/>

<div align = "center"> <img src = "https://drive.google.com/uc?id=10EQriQtNG910JkT8DCoV6eNOxgfzfTsN" width="250"> </div>

<br/>

### 4. Group Normalization

<br/>

`Group Normalization`은 앞서 설명한 `Instance Normalization`과 유사하게 동작합니다. Group Normalization은 채널을 그룹으로 묶어 평균과 표준편차를 구하여 연산한다는 차이가 있습니다. 예를 들어서, 채널이 6개 존재하고 그룹이 2이면 한 그룹당 3개의 채널이 존재하는 셈입니다. 아래의 그림은 Group Normalization 과정을 도식화한 그림입니다.

<br/>

<div align = "center"> <img src = "https://drive.google.com/uc?id=1JPvJBYflMmubdQ0cH0u8QkDFfTll9Uoc" width="300"> </div>

<br/>

또한, 아래의 그림은 [Group Normalization](https://arxiv.org/abs/1803.08494) 논문에서 Group Normalization을 수식으로 정리하여 나타낸 그림입니다. 이 때, x는 layer에 의해 계산되어지는 Feature(특성), i는 인덱스를 나타냅니다. 

<div align = "center"> <img src = "https://drive.google.com/uc?id=10-OWYREWiGnW0zxV3XIpdFXfUPUmSYLJ" width="300"> </div>

